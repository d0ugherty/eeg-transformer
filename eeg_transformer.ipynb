{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy.signal as signal\n",
    "import mne\n",
    "import random\n",
    "import torchvision\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision.datasets import DatasetFolder\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "from mne import preprocessing, Epochs\n",
    "import utils\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Band pass filtering and events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_file_path = 'data/eeg_data_A/'\n",
    "eeg_training_files = glob.glob('data/eeg_data_A/A0*T.gdf')\n",
    "\n",
    "eeg_eval_files = glob.glob(os.path.join(eeg_file_path, 'A0*E.gdf'))\n",
    "\n",
    "eeg_train_obj, epoch_train_obj = utils.band_pass_filter(eeg_training_files)\n",
    "eeg_eval_obj, epoch_eval_obj = utils.band_pass_filter(eeg_eval_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert raw data to PyTorch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_data = utils.raw_to_tensor(eeg_train_obj)\n",
    "eeg_test_data = utils.raw_to_tensor(eeg_eval_obj)\n",
    "split_size = 1000  \n",
    "\n",
    "smaller_tensors = []\n",
    "test_tensors = []\n",
    "\n",
    "for tensor in eeg_data: \n",
    "    splits = utils.split_tensor(tensor, split_size)\n",
    "    smaller_tensors.extend(splits)\n",
    "\n",
    "for tensor in eeg_test_data:  \n",
    "    splits = utils.split_tensor(tensor, split_size)\n",
    "    test_tensors.extend(splits)\n",
    "\n",
    "print(len(smaller_tensors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User-defined EEGDataset class to work with the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "eeg_data= EEGDataset(smaller_tensors)\n",
    "eeg_test_data = EEGDataset(test_tensors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augment the data for better generalization, and also for for a larger dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aug_data = []\n",
    "\n",
    "for tensor in eeg_data:\n",
    "    aug_data.append(tensor)\n",
    "    shifted_tensor = utils.time_shift(tensor.clone(), shift=40)\n",
    "    aug_data.append(shifted_tensor)\n",
    "\n",
    "    noisy_tensor = utils.add_noise(tensor.clone(), noise_level=0.9)\n",
    "    aug_data.append(noisy_tensor)\n",
    "\n",
    "    warped_tensor = utils.time_warp(tensor.clone(), factor=0.5)\n",
    "    aug_data.append(warped_tensor)\n",
    "\n",
    "\n",
    "\n",
    "max_length = max(tensor.shape[1] for tensor in aug_data)\n",
    "\n",
    "def pad_tensor(tensor, max_length):\n",
    "    padding_size = max_length - tensor.shape[1]\n",
    "    if padding_size > 0:\n",
    "        return torch.nn.functional.pad(tensor, (0, padding_size))\n",
    "    return tensor\n",
    "\n",
    "padded_tensors = [pad_tensor(torch.tensor(tensor, dtype=torch.float32), max_length) for tensor in aug_data]\n",
    "\n",
    "aug_data_set = EEGDataset(padded_tensors)\n",
    "\n",
    "eeg_data = EEGDataset(aug_data_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_train_set, eeg_val_set = random_split(eeg_data, [7500,3396]) \n",
    "BATCH_SIZE = 70\n",
    "train_loader = DataLoader(eeg_train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(eeg_val_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(eeg_test_data, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional network class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "embed_size = 40\n",
    "nhead = 10 \n",
    "num_layers = 6  \n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, dropout_rate = 0.5):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 40, (1, 4), (1, 1))\n",
    "        self.elu1 = nn.ELU()\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(40, 40, (25, 1), (1, 1))\n",
    "        self.elu2 = nn.ELU()\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "      \n",
    "        # after conv2, the height is 1\n",
    "        output_size = 40 * 1 * 997  # 40 channels, height 1, width 997\n",
    "        self.fc1 = nn.Linear(output_size, embed_size)  # adjusted for the correct input size\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.unsqueeze(1) #becomes [N, 1, 25, 1000]\n",
    "        x = self.conv1(x)\n",
    "        x = self.elu1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.elu2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc1(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_net = ConvNet().to(device)\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=embed_size, nhead=nhead,dropout=0.5, dim_feedforward=4).to(device)\n",
    "decoder_layer = nn.TransformerDecoderLayer(d_model=embed_size, nhead=nhead, dropout=0.5, dim_feedforward=4).to(device)\n",
    "\n",
    "\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers).to(device)\n",
    "transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers).to(device)\n",
    "model_params = list(transformer_encoder.parameters()) + list(transformer_decoder.parameters())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.L1Loss()    \n",
    "optimizer = optim.SGD(model_params, \n",
    "                        lr=0.001, \n",
    "                        momentum=0.9)\n",
    "\n",
    "scheduler = LinearLR(optimizer, start_factor=1.0, end_factor=0.25, total_iters=10)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "losses = []\n",
    "n = 1\n",
    "\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    #transformer.train()\n",
    "\n",
    "    for batch in train_loader: \n",
    "        src_data = batch\n",
    "        src_data = src_data.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        src_data = conv_net(src_data)\n",
    "       \n",
    "        memory = transformer_encoder(src_data)\n",
    "        \n",
    "        out_batch = transformer_decoder(src_data, memory)\n",
    "        loss = criterion(out_batch, src_data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        n += 1\n",
    "    \n",
    "    val_loss = 0.0\n",
    "    for batch in val_loader:\n",
    "        src_data = batch\n",
    "        src_data = src_data.cuda()\n",
    "        src_data = conv_net(src_data)\n",
    "    \n",
    "\n",
    "        memory = transformer_encoder(src_data)\n",
    "\n",
    "        out_batch = transformer_decoder(src_data, memory)\n",
    "        loss = criterion(out_batch, src_data)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    print(\"Epoch: {} Train Loss: {} Val Loss: {}\".format(\n",
    "                  epoch, \n",
    "                  train_loss/len(train_loader), \n",
    "                  val_loss/len(val_loader)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = 0.0\n",
    "\n",
    "with torch.no_grad():  \n",
    "    for batch in test_loader:  \n",
    "        src_data = batch\n",
    "        src_data = src_data.cuda()\n",
    "\n",
    "        \n",
    "        src_data = conv_net(src_data)  \n",
    "        memory = transformer_encoder(src_data)\n",
    "        out_batch = transformer_decoder(src_data, memory)\n",
    "\n",
    "        \n",
    "        loss = criterion(out_batch, src_data)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "\n",
    "print(f\"Test Loss: {avg_test_loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
