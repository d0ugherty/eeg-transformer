{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy.signal as signal\n",
    "import mne\n",
    "import random\n",
    "import torchvision\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision.datasets import DatasetFolder\n",
    "\n",
    "from mne import preprocessing, Epochs\n",
    "import utils\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Band pass filtering and events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_file_path = 'data/eeg_data_A/'\n",
    "eeg_training_files = glob.glob('data/eeg_data_A/A0*T.gdf')\n",
    "\n",
    "#eeg_training_files = random.sample(eeg_training_files,2)\n",
    "\n",
    "eeg_eval_files = glob.glob(os.path.join(eeg_file_path, 'A0*E.gdf'))\n",
    "\n",
    "eeg_train_obj, epoch_train_obj = utils.band_pass_filter(eeg_training_files)\n",
    "eeg_eval_obj, epoch_eval_obj = utils.band_pass_filter(eeg_eval_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_data = utils.raw_to_tensor(eeg_train_obj)\n",
    "\n",
    "eeg_test_data = utils.raw_to_tensor(eeg_eval_obj)\n",
    "\n",
    "#eeg_data = np.transpose(eeg_data, (2,1,0))\n",
    "#eeg_data = np.expand_dims(eeg_data, axis=1)\n",
    "\n",
    "eeg_test_set = np.transpose(eeg_test_data, (2,1,0))\n",
    "#eeg_test_set = np.expand_dims(eeg_test_set, axis=1)\n",
    "\n",
    "split_size = 1000  # Define your desired split size\n",
    "smaller_tensors = []\n",
    "\n",
    "for tensor in eeg_data:  # Assuming eeg_data is your original dataset\n",
    "    splits = utils.split_tensor(tensor, split_size)\n",
    "    smaller_tensors.extend(splits)\n",
    "\n",
    "print(len(smaller_tensors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "eeg_data= EEGDataset(smaller_tensors)\n",
    "\n",
    "for tensor in eeg_data:\n",
    "    tensor = utils.time_shift(tensor, shift=10)\n",
    "    tensor = utils.add_noise(tensor, noise_level=0.1)\n",
    "    tensor = utils.time_warp(tensor, factor=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max(tensor.shape[0] for tensor in eeg_data)\n",
    "\"\"\"\n",
    "padded_tensors = []\n",
    "for tensor in eeg_data:\n",
    "    padding_size = max_length - tensor.shape[0]\n",
    "    if padding_size > 0:\n",
    "        padded_tensor = torch.nn.functional.pad(tensor, (0, 0, 0, 0, 0, padding_size))\n",
    "    else:\n",
    "        padded_tensor = tensor\n",
    "    padded_tensors.append(padded_tensor)\n",
    "\n",
    "    split_ratio = 0.66  \n",
    "\"\"\"\n",
    "# Use the split_data function from utils.py\n",
    "##eeg_train_set, eeg_val_set = utils.split_data(eeg_data, eeg_training_files, .8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "eeg_train_set, eeg_val_set = random_split(eeg_data, [2500, 224]) \n",
    "\n",
    "\n",
    "print(\"Number of items in training set:\", len(eeg_train_set))\n",
    "print(\"Shape of first item in training set:\", eeg_train_set[0].shape)\n",
    "\n",
    "print(\"Number of items in validation set:\", len(eeg_val_set))\n",
    "print(\"Shape of first item in validation set:\", eeg_val_set[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "BATCH_SIZE = 70\n",
    "\n",
    "train_loader = DataLoader(eeg_train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(eeg_val_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(eeg_test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "for i, batch in enumerate(train_loader):\n",
    "    # Print the batch contents\n",
    "    print(f\"Batch {i}:\")\n",
    "    print(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "embed_size = 25\n",
    "nhead = 5  \n",
    "num_layers = 1  \n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 40, (1, 4), (1, 1))\n",
    "        self.conv2 = nn.Conv2d(40, 40, (25, 1), (1, 1))\n",
    "        self.elu1 = nn.ELU()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        # Assuming the width after conv1 is 1000 - 4 + 1 = 997\n",
    "        # After conv2, the height is 1\n",
    "        output_size = 40 * 1 * 997  # 40 channels, height 1, width 997\n",
    "        self.fc1 = nn.Linear(output_size, embed_size)  # Adjusted for the correct input size\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.unsqueeze(1) #becomes [N, 1, 25, 1000]\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.elu1(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#transformer = nn.Transformer(d_model=d_model, nhead=nhead).to(device)\n",
    "\n",
    "conv_net = ConvNet().to(device)\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=embed_size, nhead=nhead).to(device)\n",
    "decoder_layer = nn.TransformerDecoderLayer(d_model=embed_size, nhead=nhead).to(device)\n",
    "\n",
    "\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers).to(device)\n",
    "transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers).to(device)\n",
    "model_params = list(transformer_encoder.parameters()) + list(transformer_decoder.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.MSELoss()    \n",
    "optimizer = optim.SGD(model_params, \n",
    "                        lr=0.001, \n",
    "                        momentum=0.9)\n",
    "\n",
    "losses = []\n",
    "n = 1\n",
    "for epoch in range(100):\n",
    "    train_loss = 0.0\n",
    "    #transformer.train()\n",
    "\n",
    "    for batch in train_loader:\n",
    "        #print(f'Starting batch {n}')   \n",
    "        src_data = batch\n",
    "        src_data = src_data.cuda()\n",
    "        # print(src_data.shape)\n",
    "        #src_data = src_data.permute(2, 0, 1)\n",
    "        #src_data = src_data.flatten(start_dim=1)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #print(src_data.shape) \n",
    "        #print(src_data)\n",
    "        #print(\"ConvNet...\")\n",
    "        src_data = conv_net(src_data)\n",
    "        #print(src_data.shape)\n",
    "        #print(\"Encoding...\")\n",
    "        memory = transformer_encoder(src_data)\n",
    "        #print(\"Decoding...\")\n",
    "        out_batch = transformer_decoder(src_data, memory)\n",
    "        loss = criterion(out_batch, src_data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        n += 1\n",
    "        #losses.append(loss.item())\n",
    "    \n",
    "    val_loss = 0.0\n",
    "    for batch in val_loader:\n",
    "        src_data = batch\n",
    "        src_data = src_data.cuda()\n",
    "        src_data = conv_net(src_data)\n",
    "    \n",
    "\n",
    "        memory = transformer_encoder(src_data)\n",
    "        #print(\"Validation Decoding...\")\n",
    "        out_batch = transformer_decoder(src_data, memory)\n",
    "        loss = criterion(out_batch, src_data)\n",
    "       \n",
    "        val_loss += loss.item()\n",
    "\n",
    "   \n",
    "\n",
    "    print(\"Epoch: {} Train Loss: {} Val Loss: {}\".format(\n",
    "                  epoch, \n",
    "                  train_loss/len(train_loader), \n",
    "                  val_loss/len(val_loader)))\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss per Batch')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attetnion Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
